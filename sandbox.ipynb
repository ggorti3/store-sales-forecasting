{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deck/Documents/kaggle/store-sales-forecasting/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "train_df = train_df.rename({\"date\":\"ds\", \"sales\":\"y\"}, axis=1)\n",
    "\n",
    "oil_df = pd.read_csv(\"oil.csv\")\n",
    "oil_df = oil_df.rename({\"date\":\"ds\"}, axis=1)\n",
    "\n",
    "stores_df = pd.read_csv(\"stores.csv\")\n",
    "stores_dict = stores_df.set_index(\"store_nbr\").to_dict(\"index\")\n",
    "holidays_df = pd.read_csv(\"holidays_events.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df = test_df.rename({\"date\":\"ds\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate oil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_oil_df = pd.DataFrame({\"ds\":pd.date_range(train_df[\"ds\"].min(), test_df[\"ds\"].max()).astype(\"str\")})\n",
    "oil_df = blank_oil_df.merge(oil_df, how=\"left\", on=\"ds\")\n",
    "oil_df[\"dcoilwtico\"] = oil_df[\"dcoilwtico\"].interpolate(\"nearest\")\n",
    "oil_df.iloc[0, 1] = 93.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Holidays DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_496438/2455401216.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  th_df[\"description\"] = th_df[\"description\"].str.removeprefix(\"Traslado \")\n"
     ]
    }
   ],
   "source": [
    "nth_df = holidays_df[holidays_df[\"transferred\"] == False]\n",
    "th_df = holidays_df[holidays_df[\"type\"] == \"Transfer\"]\n",
    "th_df[\"description\"] = th_df[\"description\"].str.removeprefix(\"Traslado \")\n",
    "all_holidays_df = pd.concat([nth_df, th_df], axis=0)[[\"date\", \"locale_name\", \"description\"]]\n",
    "all_holidays_df = all_holidays_df.rename({\"date\":\"ds\", \"description\":\"holiday\"}, axis=1)\n",
    "all_holidays_df[\"lower_window\"] = 0\n",
    "all_holidays_df[\"upper_window\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Additional Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(oil_df, how=\"left\", on=\"ds\")\n",
    "test_df = test_df.merge(oil_df, how=\"left\", on=\"ds\")\n",
    "\n",
    "# info cols\n",
    "train_df = train_df.merge(stores_df[[\"store_nbr\", \"cluster\"]], how=\"left\", on=\"store_nbr\")\n",
    "test_df = test_df.merge(stores_df[[\"store_nbr\", \"cluster\"]], how=\"left\", on=\"store_nbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_counts = train_df.groupby([\"store_nbr\", \"family\"])[\"ds\"].agg(\"count\")\n",
    "\n",
    "stores = pd.unique(train_df[\"store_nbr\"])\n",
    "families = pd.unique(train_df[\"family\"])\n",
    "\n",
    "promotion_counts = train_df[[\"id\", \"onpromotion\"]].groupby(\"onpromotion\").agg(\"count\")\n",
    "num_promotions = promotion_counts.shape[0]\n",
    "\n",
    "states = pd.unique(stores_df[\"state\"])\n",
    "cities = pd.unique(stores_df[\"city\"])\n",
    "types = pd.unique(stores_df[\"type\"])\n",
    "cities_per_state = stores_df[[\"state\", \"city\"]].drop_duplicates().groupby(\"state\").agg(\"count\")\n",
    "stores_per_city = stores_df[[\"city\", \"store_nbr\"]].drop_duplicates().groupby(\"city\").agg(\"count\")\n",
    "stores_per_cluster = stores_df[[\"cluster\", \"store_nbr\"]].drop_duplicates().groupby(\"cluster\").agg(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AUTOMOTIVE', 'BABY CARE', 'BEAUTY', 'BEVERAGES', 'BOOKS',\n",
       "       'BREAD/BAKERY', 'CELEBRATION', 'CLEANING', 'DAIRY', 'DELI', 'EGGS',\n",
       "       'FROZEN FOODS', 'GROCERY I', 'GROCERY II', 'HARDWARE',\n",
       "       'HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES',\n",
       "       'HOME CARE', 'LADIESWEAR', 'LAWN AND GARDEN', 'LINGERIE',\n",
       "       'LIQUOR,WINE,BEER', 'MAGAZINES', 'MEATS', 'PERSONAL CARE',\n",
       "       'PET SUPPLIES', 'PLAYERS AND ELECTRONICS', 'POULTRY',\n",
       "       'PREPARED FOODS', 'PRODUCE', 'SCHOOL AND OFFICE SUPPLIES',\n",
       "       'SEAFOOD'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset as necessary for implementation and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[train_df[\"store_nbr\"] == 1]\n",
    "# test_df = test_df[test_df[\"store_nbr\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet: one model per store, family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msle(preds_df):\n",
    "    return np.mean((np.log(1 + preds_df[\"y\"].values) - np.log(1 + preds_df[\"yhat\"].values))**2)\n",
    "\n",
    "def ssle(preds_df):\n",
    "    return np.sum((np.log(1 + preds_df[\"y\"].values) - np.log(1 + preds_df[\"yhat\"].values))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bottom_up(key_cols, train_df, stores_dict, all_holidays_df):\n",
    "    def fit(x_df):\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "\n",
    "        model = Prophet(uncertainty_samples=0, holidays=h_df)\n",
    "        model.add_regressor(\"onpromotion\")\n",
    "        model.add_regressor(\"dcoilwtico\")\n",
    "\n",
    "        model.fit(x_df)\n",
    "        return model\n",
    "    \n",
    "    return train_df.groupby(key_cols).apply(fit).reset_index()\n",
    "\n",
    "def predict_bottom_up(test_df, models_df):\n",
    "    key_cols = models_df.columns[:-1].to_list()\n",
    "    def predict(x_df):\n",
    "        filter = pd.Series(models_df.shape[0] * [True])\n",
    "        for k in key_cols:\n",
    "            v = x_df[k].iloc[0]\n",
    "            filter = filter & (models_df[k] == v)\n",
    "        model = models_df[filter].iloc[0, -1]\n",
    "\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "\n",
    "        return model.predict(x_df)\n",
    "\n",
    "    return test_df.groupby(key_cols).apply(predict).reset_index()\n",
    "\n",
    "def all_cross_validation(key_cols, train_df, stores_dict, all_holidays_df):\n",
    "    def cv(x_df):\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "\n",
    "\n",
    "        model = Prophet(uncertainty_samples=0, holidays=h_df)\n",
    "        model.add_regressor(\"onpromotion\")\n",
    "        model.add_regressor(\"dcoilwtico\")\n",
    "        model.fit(x_df)\n",
    "        cv_df = cross_validation(model, initial='1460 days', period='56 days', horizon='16 days')\n",
    "        cv_df[\"yhat\"] = cv_df[\"yhat\"].clip(lower=0)\n",
    "        return cv_df.groupby(\"cutoff\").apply(msle).reset_index()\n",
    "\n",
    "    msles_df = train_df.groupby(key_cols).apply(cv).reset_index().rename({0:\"msle\"}, axis=1)\n",
    "    return np.mean(msles_df.groupby(\"cutoff\")[\"msle\"].agg(\"mean\").values)**0.5\n",
    "\n",
    "def cv_optimize(key_cols, train_df, stores_dict, all_holidays_df, hparam_grid):\n",
    "    def cv_opt(x_df):\n",
    "        # get h_df\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        # aggregate x_df\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "\n",
    "        # hparam dict\n",
    "        best_hparams = {hp:hparam_grid[hp][0] for hp in hparam_grid.keys()}\n",
    "        # coord descent loop over hparam grid\n",
    "        min_err = float(\"inf\")\n",
    "        for i, hp in enumerate(hparam_grid.keys()):\n",
    "            # prevent redundant cvs\n",
    "            grid = hparam_grid[hp] if i == 0 else hparam_grid[hp][1:]\n",
    "            for val in grid:\n",
    "                hparams = best_hparams.copy()\n",
    "                hparams[hp] = val\n",
    "                model = Prophet(uncertainty_samples=0, holidays=h_df, **hparams)\n",
    "                model.add_regressor(\"onpromotion\")\n",
    "                model.add_regressor(\"dcoilwtico\")\n",
    "                model.fit(x_df)\n",
    "                cv_df = cross_validation(model, initial='1460 days', period='56 days', horizon='16 days')\n",
    "                cv_df[\"yhat\"] = cv_df[\"yhat\"].clip(lower=0)\n",
    "                msles = cv_df.groupby(\"cutoff\").apply(msle).values\n",
    "                err = np.mean(msles)\n",
    "                if err < min_err:\n",
    "                    best_hparams[hp] = val\n",
    "                    min_err = err\n",
    "        df_dict = {hp:[best_hparams[hp]] for hp in best_hparams.keys()}\n",
    "        df_dict[\"msle\"] = [min_err]\n",
    "        return pd.DataFrame(df_dict)\n",
    "\n",
    "    best_hparams_df = train_df.groupby(key_cols).apply(cv_opt).reset_index()\n",
    "    return best_hparams_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down-Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportions(key_cols, support_cols, x_df):\n",
    "    x_df = x_df.groupby(key_cols + support_cols + [\"ds\"]).agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "    agg_x_df = x_df.groupby(key_cols + [\"ds\"]).agg({\"y\":\"sum\"}).reset_index().rename({\"y\":\"agg_y\"}, axis=1)\n",
    "    x_df = x_df.merge(agg_x_df, how=\"left\", on=key_cols + [\"ds\"])\n",
    "    x_df[\"prop\"] = x_df.loc[:, [\"y\"]].where(x_df[\"agg_y\"] <= 0, x_df[\"y\"] / x_df[\"agg_y\"], axis=0)\n",
    "    return x_df.drop([\"y\", \"agg_y\"], axis=1)\n",
    "\n",
    "def year_diff(key_cols, support_cols, prop_df):\n",
    "    other_df = prop_df.copy()\n",
    "    other_df[\"ds\"] = (pd.to_datetime(other_df[\"ds\"]) + DateOffset(years=1)).dt.strftime(\"%Y-%m-%d\")\n",
    "    other_df = other_df.groupby(key_cols + support_cols + [\"ds\"]).agg(\"first\").reset_index()\n",
    "    other_df = other_df.rename({\"prop\":\"prev_prop\"}, axis=1)[key_cols + support_cols + [\"ds\", \"prev_prop\"]]\n",
    "    leap_yrs_df = other_df[other_df[\"ds\"] == \"2016-02-28\"]\n",
    "    leap_yrs_df[\"ds\"] = \"2016-02-29\"\n",
    "    other_df = pd.concat([other_df, leap_yrs_df]).sort_values(key_cols + support_cols + [\"ds\"]).reset_index(drop=True)\n",
    "    dprop_df = prop_df.merge(other_df, how=\"inner\", on=key_cols + support_cols + [\"ds\"])\n",
    "    dprop_df[\"prop\"] = dprop_df[\"prop\"] - dprop_df[\"prev_prop\"]\n",
    "    dprop_df = dprop_df[key_cols + support_cols + [\"ds\", \"prop\", \"onpromotion\", \"dcoilwtico\"]]\n",
    "    return dprop_df, other_df\n",
    "\n",
    "def year_inv_diff(key_cols, support_cols, prop_hat_df, other_df):\n",
    "    prop_hat_df = prop_hat_df.merge(other_df, how=\"inner\", on=key_cols + support_cols + [\"ds\"])\n",
    "    prop_hat_df[\"prop_hat\"] = prop_hat_df[\"prop_hat\"] + prop_hat_df[\"prev_prop\"]\n",
    "    prop_hat_df = prop_hat_df[key_cols + support_cols + [\"ds\", \"prop_hat\"]]\n",
    "    return prop_hat_df\n",
    "\n",
    "class VARModel():\n",
    "    def __init__(self, lag, support_cols):\n",
    "        self.lag = lag\n",
    "        self.support_cols = support_cols\n",
    "    \n",
    "    def fit(self, x_df, lmbda):\n",
    "        px_df = x_df.pivot(columns=self.support_cols, index=\"ds\", values=\"prop\").sort_index()\n",
    "        oil_df = x_df[[\"ds\", \"dcoilwtico\"]].drop_duplicates().sort_values(\"ds\")\n",
    "        # need to use loop to build design matrix\n",
    "        design_cols = []\n",
    "        for l in range(self.lag):\n",
    "            dc = px_df.iloc[l:(px_df.shape[0] - self.lag + l), :].values.flatten()\n",
    "            design_cols.append(dc)\n",
    "        design_cols.append(np.repeat(oil_df[\"dcoilwtico\"].values[(self.lag):], px_df.shape[1]))\n",
    "        X = np.stack(design_cols, axis=1)\n",
    "        y = px_df.iloc[self.lag:, :].values.flatten()\n",
    "        self.beta = lin_reg(X, y, lmbda)\n",
    "\n",
    "        self.px = px_df.values[-self.lag:, :].T\n",
    "        self.d = datetime.strptime(px_df.index[-1], \"%Y-%m-%d\").date() + timedelta(days=1)\n",
    "        self.support = px_df.columns\n",
    "    \n",
    "    def predict(self, test_oil_df, days):\n",
    "        test_oil_df = test_oil_df[[\"ds\", \"dcoilwtico\"]].drop_duplicates().set_index(\"ds\").sort_index()\n",
    "        ox = np.full((self.px.shape[0], 1), test_oil_df.loc[self.d.strftime(\"%Y-%m-%d\"), \"dcoilwtico\"])\n",
    "        bx = np.ones((self.px.shape[0], 1))\n",
    "        x = np.concatenate([bx, self.px, ox], axis=1)\n",
    "        d0 = self.d\n",
    "        out = []\n",
    "        for i in range(days):\n",
    "            if i > 0:\n",
    "                self.px = np.concatenate([self.px[:, 1:], y[:, np.newaxis]], axis=1)\n",
    "                self.d = self.d + timedelta(days=1)\n",
    "                ox = np.full((self.px.shape[0], 1), test_oil_df.loc[self.d.strftime(\"%Y-%m-%d\"), \"dcoilwtico\"])\n",
    "                bx = np.ones((self.px.shape[0], 1))\n",
    "                x = np.concatenate([bx, self.px, ox], axis=1)\n",
    "                \n",
    "            y = x @ self.beta\n",
    "            out.append(y)\n",
    "        ds = pd.date_range(start=d0.strftime(\"%Y-%m-%d\"), periods=days, freq=\"D\", inclusive=\"left\").repeat(self.px.shape[0]).strftime(\"%Y-%m-%d\")\n",
    "        pdf_dict = {\"ds\":ds}\n",
    "        if len(self.support_cols) == 1:\n",
    "            pdf_dict[self.support_cols[0]] = np.tile(self.support.to_numpy(), (days, ))\n",
    "        else:\n",
    "            for j, sc in enumerate(self.support_cols):\n",
    "                pdf_dict[sc] = np.tile(np.array([self.support[i][j] for i in range(len(self.support))]), (days, ))\n",
    "            pass\n",
    "        pdf_dict[\"prop_hat\"] = np.concatenate(out)\n",
    "        preds_df = pd.DataFrame(pdf_dict)\n",
    "        return preds_df\n",
    "\n",
    "\n",
    "def lin_reg(X, y, lmbda):\n",
    "    X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "    beta = np.linalg.solve(X.T @ X + lmbda * np.eye(X.shape[1]), X.T @ y)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_VAR(key_cols, support_cols, train_df):\n",
    "    prop_df = get_proportions(key_cols, support_cols, train_df)\n",
    "    prop_df, prev_prop_df = year_diff(key_cols, support_cols, prop_df)\n",
    "\n",
    "    def fit(x_df):\n",
    "        model = VARModel(lag=6, support_cols=support_cols)\n",
    "        model.fit(x_df, 0)\n",
    "        return model\n",
    "    return prop_df.groupby(key_cols).apply(fit).reset_index(), prev_prop_df\n",
    "\n",
    "\n",
    "def predict_VAR(key_cols, support_cols, test_df, var_models_df, prev_prop_df):\n",
    "    def predict(x_df):\n",
    "        days = x_df[\"ds\"].drop_duplicates().shape[0]\n",
    "        filter = pd.Series(var_models_df.shape[0] * [True])\n",
    "        for k in key_cols:\n",
    "            v = x_df[k].iloc[0]\n",
    "            filter = filter & (var_models_df[k] == v)\n",
    "        model = var_models_df[filter].iloc[0, -1]\n",
    "        return model.predict(test_df, days)\n",
    "\n",
    "    prop_hat_df = test_df.groupby(key_cols).apply(predict).reset_index()\n",
    "    return year_inv_diff(key_cols, support_cols, prop_hat_df, prev_prop_df)\n",
    "\n",
    "def top_down_cv(key_cols, support_cols, train_df, stores_dict, all_holidays_df, train_days=365*3, val_days=16, interval=64):\n",
    "    d0 = datetime.strptime(train_df[\"ds\"].iloc[0], \"%Y-%m-%d\").date()\n",
    "    d_max = datetime.strptime(train_df[\"ds\"].iloc[-1], \"%Y-%m-%d\").date()\n",
    "\n",
    "    def cv(x_df):\n",
    "        # x_df has all same key_cols but not grouped yet, contains y, dprop, and prev_prop\n",
    "\n",
    "        # retrieve h_df\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        # aggregate x_df on key and support cols\n",
    "        x_df = x_df.groupby(key_cols + support_cols + [\"ds\"]).agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index().sort_values(key_cols + support_cols + [\"ds\"])\n",
    "        # get proportions and transform\n",
    "        x_prop_df = get_proportions(key_cols, support_cols, x_df)\n",
    "        x_prop_df, x_prev_prop_df = year_diff(key_cols, support_cols, x_prop_df)\n",
    "        x_prop_df = x_prop_df.sort_values(key_cols + support_cols + [\"ds\"])\n",
    "        x_prev_prop_df = x_prev_prop_df.sort_values(key_cols + support_cols + [\"ds\"])\n",
    "\n",
    "        # internal loop for each fold\n",
    "        # for loop\n",
    "        d = d0\n",
    "        errs = []\n",
    "        while d + timedelta(days=train_days) + timedelta(days=val_days - 1) <= d_max:\n",
    "            # calculate train dates and test dates\n",
    "            d1 = d.strftime(\"%Y-%m-%d\")\n",
    "            d2 = (d + timedelta(days=train_days)).strftime(\"%Y-%m-%d\")\n",
    "            d3 = (d + timedelta(days=train_days + val_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # retrieve agg train ys for prophet training\n",
    "            x_agg_train_df = x_df[(x_df[\"ds\"] >= d1) & (x_df[\"ds\"] < d2)].groupby(key_cols + [\"ds\"]).agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "            # fit model\n",
    "            model = Prophet(uncertainty_samples=0, holidays=h_df)\n",
    "            model.add_regressor(\"onpromotion\")\n",
    "            model.add_regressor(\"dcoilwtico\")\n",
    "            model.fit(x_agg_train_df)\n",
    "            # retrieve agg val df for propher prediction\n",
    "            x_agg_val_df = x_df[(x_df[\"ds\"] >= d2) & (x_df[\"ds\"] < d3)].groupby(key_cols + [\"ds\"]).agg({\"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "            # predict\n",
    "            x_pred_df = model.predict(x_agg_val_df)[[\"ds\", \"yhat\"]]\n",
    "            x_pred_df[\"ds\"] = np.datetime_as_string(x_pred_df[\"ds\"].to_numpy(), unit='D')\n",
    "\n",
    "            # retrieve train props for var training\n",
    "            x_prop_train_df = x_prop_df[(x_prop_df[\"ds\"] >= d1) & (x_prop_df[\"ds\"] < d2)]\n",
    "            # fit var model and predict\n",
    "            var_model = VARModel(lag=8, support_cols=support_cols)\n",
    "            var_model.fit(x_prop_train_df, 1e-1)\n",
    "            x_prop_hat_df = var_model.predict(x_prop_df[(x_prop_df[\"ds\"] >= d2) & (x_prop_df[\"ds\"] < d3)], val_days)\n",
    "            # transform var predictions using prev_props\n",
    "            x_prop_hat_df = year_inv_diff([], support_cols, x_prop_hat_df, x_prev_prop_df)\n",
    "\n",
    "            # merge predictions and multiply to get final predictions\n",
    "            x_pred_df = x_prop_hat_df.merge(x_pred_df, on=\"ds\", how=\"left\")\n",
    "            x_pred_df[\"yhat\"] = (x_pred_df[\"yhat\"] * x_pred_df[\"prop_hat\"])\n",
    "            x_pred_df = x_pred_df.drop(\"prop_hat\", axis=1)\n",
    "            x_pred_df[\"yhat\"] = x_pred_df[\"yhat\"].clip(lower=0)\n",
    "\n",
    "            true_df = x_df[(x_df[\"ds\"] >= d2) & (x_df[\"ds\"] < d3)].drop(key_cols + [\"onpromotion\", \"dcoilwtico\"], axis=1)\n",
    "            x_pred_df = x_pred_df.merge(true_df, how=\"left\", on=support_cols + [\"ds\"])\n",
    "\n",
    "            err = msle(x_pred_df)\n",
    "            errs.append(err)\n",
    "\n",
    "            # calculate rmsle\n",
    "\n",
    "            # increment date\n",
    "            d += timedelta(days=interval)\n",
    "        return np.mean(np.array(errs))\n",
    "\n",
    "        # merge prophet predictions and var predictions, then multiply to retrieve final prediction\n",
    "    return train_df.groupby(key_cols).apply(cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_down_cv_prophet(key_cols, support_cols, train_df, stores_dict, all_holidays_df, train_days=365*3, val_days=16, interval=64):\n",
    "    # define fit_predict function for level 1\n",
    "    def fit_predict(x_df, d1, d2, d3):\n",
    "        # retrieve h_df\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        # aggregate x_df on ds and key cols (already has all same key_cols so this is implicit)\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index().sort_values(\"ds\")\n",
    "\n",
    "        train_filter = (x_df[\"ds\"] >= d1) & (x_df[\"ds\"] < d2)\n",
    "        val_filter = (x_df[\"ds\"] >= d2) & (x_df[\"ds\"] < d3)\n",
    "\n",
    "        # fit prophet model on train data\n",
    "        model = Prophet(uncertainty_samples=0, holidays=h_df)\n",
    "        model.add_regressor(\"onpromotion\")\n",
    "        model.add_regressor(\"dcoilwtico\")\n",
    "        model.fit(x_df[train_filter])\n",
    "\n",
    "        # predict over val dates\n",
    "        x_pred_df = model.predict(x_df[val_filter][[\"ds\", \"onpromotion\", \"dcoilwtico\"]])[[\"ds\", \"yhat\"]]\n",
    "        x_pred_df[\"ds\"] = np.datetime_as_string(x_pred_df[\"ds\"].to_numpy(), unit='D')\n",
    "        \n",
    "        return x_pred_df\n",
    "        \n",
    "\n",
    "    # define fit_predict function for props\n",
    "    def fit_predict_prop(x_df, d1, d2, d3):\n",
    "        # x_df is all same key and support cols, has onpromotion and dcoilwtico\n",
    "        x_df = x_df.rename({\"prop\":\"y\"}, axis=1)\n",
    "\n",
    "        # calculate train and val filter\n",
    "        train_filter = (x_df[\"ds\"] >= d1) & (x_df[\"ds\"] < d2)\n",
    "        val_filter = (x_df[\"ds\"] >= d2) & (x_df[\"ds\"] < d3)\n",
    "\n",
    "        # fit prophet model\n",
    "        model = Prophet(uncertainty_samples=0)\n",
    "        model.add_regressor(\"onpromotion\")\n",
    "        model.add_regressor(\"dcoilwtico\")\n",
    "        model.fit(x_df[train_filter])\n",
    "\n",
    "        # predict over val_dates\n",
    "        x_pred_df = model.predict(x_df[val_filter][[\"ds\", \"onpromotion\", \"dcoilwtico\"]])[[\"ds\", \"yhat\"]]\n",
    "        x_pred_df[\"ds\"] = np.datetime_as_string(x_pred_df[\"ds\"].to_numpy(), unit='D')\n",
    "        x_pred_df = x_pred_df.rename({\"yhat\":\"prop_hat\"}, axis=1)\n",
    "\n",
    "        return x_pred_df\n",
    "\n",
    "    # get important dates\n",
    "    d0 = datetime.strptime(train_df[\"ds\"].iloc[0], \"%Y-%m-%d\").date()\n",
    "    d_max = datetime.strptime(train_df[\"ds\"].iloc[-1], \"%Y-%m-%d\").date()\n",
    "\n",
    "    agg_train_df = train_df.groupby(key_cols + support_cols + [\"ds\"]).agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index().sort_values(\"ds\")\n",
    "\n",
    "    # get prop df\n",
    "    prop_df = get_proportions(key_cols, support_cols, train_df)\n",
    "    prop_df = prop_df.sort_values(key_cols + support_cols + [\"ds\"])\n",
    "\n",
    "    # cv loop\n",
    "    d = d0\n",
    "    errs = []\n",
    "    while d + timedelta(days=train_days) + timedelta(days=val_days - 1) <= d_max:\n",
    "        print(\"fold {}\".format(d))\n",
    "        # calculate train dates and test dates\n",
    "        d1 = d.strftime(\"%Y-%m-%d\")\n",
    "        d2 = (d + timedelta(days=train_days)).strftime(\"%Y-%m-%d\")\n",
    "        d3 = (d + timedelta(days=train_days + val_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # call fit_predict function on both dfs\n",
    "        pred_df = train_df.groupby(key_cols).apply(lambda x: fit_predict(x, d1, d2, d3), include_groups=False).reset_index().drop(\"level_{}\".format(len(key_cols)), axis=1)\n",
    "        pred_prop_df = prop_df.groupby(key_cols + support_cols).apply(lambda x: fit_predict_prop(x, d1, d2, d3), include_groups=False).reset_index().drop(\"level_{}\".format(len(key_cols + support_cols)), axis=1)\n",
    "\n",
    "        # multiply preds together\n",
    "        pred_df = pred_prop_df.merge(pred_df, how=\"left\", on=key_cols + [\"ds\"])\n",
    "        pred_df[\"yhat\"] = pred_df[\"yhat\"] * pred_df[\"prop_hat\"]\n",
    "        pred_df[\"yhat\"] = pred_df[\"yhat\"].clip(lower=0)\n",
    "\n",
    "        # merge with truth\n",
    "        pred_df = pred_df.merge(agg_train_df, how=\"left\", on=key_cols + support_cols + [\"ds\"])\n",
    "\n",
    "        # calculate metric\n",
    "        err = msle(pred_df)**0.5\n",
    "        print(err)\n",
    "        errs.append(err)\n",
    "\n",
    "        d = d + timedelta(days=interval)\n",
    "\n",
    "    return np.mean(np.array(errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols = [\"cluster\"]\n",
    "support_cols = [\"family\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = all_cross_validation(key_cols, train_df, stores_dict, all_holidays_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = 4\n",
    "# prop_df = get_proportions(key_cols, support_cols, train_df)\n",
    "# prop_df, prev_prop_df = year_diff(key_cols, support_cols, prop_df)\n",
    "# x_df = prop_df[prop_df[\"cluster\"] == cluster]\n",
    "\n",
    "# model = VARModel(lag=6, support_cols=support_cols)\n",
    "# model.fit(x_df, 0)\n",
    "# prop_hat_df = model.predict(test_df, 16)\n",
    "# prop_hat_df[\"cluster\"] = cluster\n",
    "# year_inv_diff(key_cols, support_cols, prop_hat_df, prev_prop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_models_df, prev_prop_df = fit_VAR(key_cols, support_cols, train_df)\n",
    "# predict_VAR(key_cols, support_cols, test_df, var_models_df, prev_prop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = top_down_cv(key_cols, support_cols, train_df, stores_dict, all_holidays_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
