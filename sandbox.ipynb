{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gautham/Documents/Kaggle/store-sales-forecasting/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "train_df = train_df.rename({\"date\":\"ds\", \"sales\":\"y\"}, axis=1)\n",
    "\n",
    "oil_df = pd.read_csv(\"oil.csv\")\n",
    "oil_df = oil_df.rename({\"date\":\"ds\"}, axis=1)\n",
    "\n",
    "stores_df = pd.read_csv(\"stores.csv\")\n",
    "stores_dict = stores_df.set_index(\"store_nbr\").to_dict(\"index\")\n",
    "holidays_df = pd.read_csv(\"holidays_events.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df = test_df.rename({\"date\":\"ds\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate oil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_oil_df = pd.DataFrame({\"ds\":pd.date_range(train_df[\"ds\"].min(), test_df[\"ds\"].max()).astype(\"str\")})\n",
    "oil_df = blank_oil_df.merge(oil_df, how=\"left\", on=\"ds\")\n",
    "oil_df[\"dcoilwtico\"] = oil_df[\"dcoilwtico\"].interpolate(\"nearest\")\n",
    "oil_df.iloc[0, 1] = 93.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Holidays DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/70/9p60jp9112d1hs3c2p3vr1th0000gn/T/ipykernel_48296/2455401216.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  th_df[\"description\"] = th_df[\"description\"].str.removeprefix(\"Traslado \")\n"
     ]
    }
   ],
   "source": [
    "nth_df = holidays_df[holidays_df[\"transferred\"] == False]\n",
    "th_df = holidays_df[holidays_df[\"type\"] == \"Transfer\"]\n",
    "th_df[\"description\"] = th_df[\"description\"].str.removeprefix(\"Traslado \")\n",
    "all_holidays_df = pd.concat([nth_df, th_df], axis=0)[[\"date\", \"locale_name\", \"description\"]]\n",
    "all_holidays_df = all_holidays_df.rename({\"date\":\"ds\", \"description\":\"holiday\"}, axis=1)\n",
    "all_holidays_df[\"lower_window\"] = 0\n",
    "all_holidays_df[\"upper_window\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Additional Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(oil_df, how=\"left\", on=\"ds\")\n",
    "test_df = test_df.merge(oil_df, how=\"left\", on=\"ds\")\n",
    "\n",
    "# info cols\n",
    "train_df = train_df.merge(stores_df[[\"store_nbr\", \"cluster\"]], how=\"left\", on=\"store_nbr\")\n",
    "test_df = test_df.merge(stores_df[[\"store_nbr\", \"cluster\"]], how=\"left\", on=\"store_nbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_counts = train_df.groupby([\"store_nbr\", \"family\"])[\"ds\"].agg(\"count\")\n",
    "\n",
    "stores = pd.unique(train_df[\"store_nbr\"])\n",
    "families = pd.unique(train_df[\"family\"])\n",
    "\n",
    "promotion_counts = train_df[[\"id\", \"onpromotion\"]].groupby(\"onpromotion\").agg(\"count\")\n",
    "num_promotions = promotion_counts.shape[0]\n",
    "\n",
    "states = pd.unique(stores_df[\"state\"])\n",
    "cities = pd.unique(stores_df[\"city\"])\n",
    "types = pd.unique(stores_df[\"type\"])\n",
    "cities_per_state = stores_df[[\"state\", \"city\"]].drop_duplicates().groupby(\"state\").agg(\"count\")\n",
    "stores_per_city = stores_df[[\"city\", \"store_nbr\"]].drop_duplicates().groupby(\"city\").agg(\"count\")\n",
    "stores_per_cluster = stores_df[[\"cluster\", \"store_nbr\"]].drop_duplicates().groupby(\"cluster\").agg(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset as necessary for implementation and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df[\"store_nbr\"] < 4]\n",
    "#test_df = test_df[test_df[\"store_nbr\"] < 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet: one model per store, family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msle(preds_df):\n",
    "    return np.mean((np.log(1 + preds_df[\"y\"].values) - np.log(1 + preds_df[\"yhat\"].values))**2)\n",
    "\n",
    "def ssle(preds_df):\n",
    "    return np.sum((np.log(1 + preds_df[\"y\"].values) - np.log(1 + preds_df[\"yhat\"].values))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bottom_up(key_cols, train_df, stores_dict, all_holidays_df):\n",
    "    def fit(x_df):\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "\n",
    "        model = Prophet(uncertainty_samples=0, holidays=h_df)\n",
    "        model.add_regressor(\"onpromotion\")\n",
    "        model.add_regressor(\"dcoilwtico\")\n",
    "\n",
    "        model.fit(x_df)\n",
    "        return model\n",
    "    \n",
    "    return train_df.groupby(key_cols).apply(fit).reset_index()\n",
    "\n",
    "def predict_bottom_up(test_df, models_df):\n",
    "    key_cols = models_df.columns[:-1].to_list()\n",
    "    def predict(x_df):\n",
    "        filter = pd.Series(models_df.shape[0] * [True])\n",
    "        for k in key_cols:\n",
    "            v = x_df[k].iloc[0]\n",
    "            filter = filter & (models_df[k] == v)\n",
    "        model = models_df[filter].iloc[0, -1]\n",
    "\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "\n",
    "        return model.predict(x_df)\n",
    "\n",
    "    return test_df.groupby(key_cols).apply(predict).reset_index()\n",
    "\n",
    "def all_cross_validation(key_cols, train_df, stores_dict, all_holidays_df):\n",
    "    def cv(x_df):\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        x_df = x_df.groupby(\"ds\").agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "\n",
    "\n",
    "        model = Prophet(uncertainty_samples=0, holidays=h_df)\n",
    "        model.add_regressor(\"onpromotion\")\n",
    "        model.add_regressor(\"dcoilwtico\")\n",
    "        model.fit(x_df)\n",
    "        cv_df = cross_validation(model, initial='1460 days', period='56 days', horizon='16 days')\n",
    "        cv_df[\"yhat\"] = cv_df[\"yhat\"].clip(lower=0)\n",
    "        return cv_df.groupby(\"cutoff\").apply(msle).reset_index()\n",
    "\n",
    "    msles_df = train_df.groupby(key_cols).apply(cv).reset_index().rename({0:\"msle\"}, axis=1)\n",
    "    return np.mean(np.sqrt(msles_df.groupby(\"cutoff\")[\"msle\"].agg(\"mean\").values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down-Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportions(key_cols, support_cols, x_df):\n",
    "    x_df = x_df.groupby(key_cols + support_cols + [\"ds\"]).agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "    agg_x_df = x_df.groupby(key_cols + [\"ds\"]).agg({\"y\":\"sum\"}).reset_index().rename({\"y\":\"agg_y\"}, axis=1)\n",
    "    x_df = x_df.merge(agg_x_df, how=\"left\", on=key_cols + [\"ds\"])\n",
    "    x_df[\"prop\"] = x_df.loc[:, [\"y\"]].where(x_df[\"agg_y\"] <= 0, x_df[\"y\"] / x_df[\"agg_y\"], axis=0)\n",
    "    return x_df.drop([\"y\", \"agg_y\"], axis=1)\n",
    "\n",
    "def year_diff(key_cols, support_cols, prop_df):\n",
    "    other_df = prop_df.copy()\n",
    "    other_df[\"ds\"] = (pd.to_datetime(other_df[\"ds\"]) + DateOffset(years=1)).dt.strftime(\"%Y-%m-%d\")\n",
    "    other_df = other_df.groupby(key_cols + support_cols + [\"ds\"]).agg(\"first\").reset_index()\n",
    "    other_df = other_df.rename({\"prop\":\"prev_prop\"}, axis=1)[key_cols + support_cols + [\"ds\", \"prev_prop\"]]\n",
    "    leap_yrs_df = other_df[other_df[\"ds\"] == \"2016-02-28\"]\n",
    "    leap_yrs_df[\"ds\"] = \"2016-02-29\"\n",
    "    other_df = pd.concat([other_df, leap_yrs_df]).sort_values(key_cols + support_cols + [\"ds\"]).reset_index(drop=True)\n",
    "    dprop_df = prop_df.merge(other_df, how=\"inner\", on=key_cols + support_cols + [\"ds\"])\n",
    "    dprop_df[\"prop\"] = dprop_df[\"prop\"] - dprop_df[\"prev_prop\"]\n",
    "    dprop_df = dprop_df[key_cols + support_cols + [\"ds\", \"prop\", \"onpromotion\", \"dcoilwtico\"]]\n",
    "    return dprop_df, other_df\n",
    "\n",
    "def year_inv_diff(key_cols, support_cols, prop_hat_df, other_df):\n",
    "    prop_hat_df = prop_hat_df.merge(other_df, how=\"inner\", on=key_cols + support_cols + [\"ds\"])\n",
    "    prop_hat_df[\"prop_hat\"] = prop_hat_df[\"prop_hat\"] + prop_hat_df[\"prev_prop\"]\n",
    "    prop_hat_df = prop_hat_df[key_cols + support_cols + [\"ds\", \"prop_hat\"]]\n",
    "    return prop_hat_df\n",
    "\n",
    "class VARModel():\n",
    "    def __init__(self, lag, support_cols):\n",
    "        self.lag = lag\n",
    "        self.support_cols = support_cols\n",
    "    \n",
    "    def fit(self, x_df, lmbda):\n",
    "        px_df = x_df.pivot(columns=self.support_cols, index=\"ds\", values=\"prop\").sort_index()\n",
    "        oil_df = x_df[[\"ds\", \"dcoilwtico\"]].drop_duplicates().sort_values(\"ds\")\n",
    "        # need to use loop to build design matrix\n",
    "        design_cols = []\n",
    "        for l in range(self.lag):\n",
    "            dc = px_df.iloc[l:(px_df.shape[0] - self.lag + l), :].values.flatten()\n",
    "            design_cols.append(dc)\n",
    "        design_cols.append(np.repeat(oil_df[\"dcoilwtico\"].values[(self.lag):], px_df.shape[1]))\n",
    "        X = np.stack(design_cols, axis=1)\n",
    "        y = px_df.iloc[self.lag:, :].values.flatten()\n",
    "        self.beta = lin_reg(X, y, lmbda)\n",
    "\n",
    "        self.px = px_df.values[-self.lag:, :].T\n",
    "        self.d = datetime.strptime(px_df.index[-1], \"%Y-%m-%d\").date() + timedelta(days=1)\n",
    "        self.support = px_df.columns\n",
    "    \n",
    "    def predict(self, test_oil_df, days):\n",
    "        test_oil_df = test_oil_df[[\"ds\", \"dcoilwtico\"]].drop_duplicates().set_index(\"ds\").sort_index()\n",
    "        ox = np.full((self.px.shape[0], 1), test_oil_df.loc[self.d.strftime(\"%Y-%m-%d\"), \"dcoilwtico\"])\n",
    "        bx = np.ones((self.px.shape[0], 1))\n",
    "        x = np.concatenate([bx, self.px, ox], axis=1)\n",
    "        d0 = self.d\n",
    "        out = []\n",
    "        for i in range(days):\n",
    "            if i > 0:\n",
    "                self.px = np.concatenate([self.px[:, 1:], y[:, np.newaxis]], axis=1)\n",
    "                self.d = self.d + timedelta(days=1)\n",
    "                ox = np.full((self.px.shape[0], 1), test_oil_df.loc[self.d.strftime(\"%Y-%m-%d\"), \"dcoilwtico\"])\n",
    "                bx = np.ones((self.px.shape[0], 1))\n",
    "                x = np.concatenate([bx, self.px, ox], axis=1)\n",
    "                \n",
    "            y = x @ self.beta\n",
    "            out.append(y)\n",
    "        ds = pd.date_range(start=d0.strftime(\"%Y-%m-%d\"), periods=days, freq=\"D\", inclusive=\"left\").repeat(self.px.shape[0]).strftime(\"%Y-%m-%d\")\n",
    "        pdf_dict = {\"ds\":ds}\n",
    "        if len(self.support_cols) == 1:\n",
    "            pdf_dict[self.support_cols[0]] = np.tile(self.support.to_numpy(), (days, ))\n",
    "        else:\n",
    "            for j, sc in enumerate(self.support_cols):\n",
    "                pdf_dict[sc] = np.tile(np.array([self.support[i][j] for i in range(len(self.support))]), (days, ))\n",
    "            pass\n",
    "        pdf_dict[\"prop_hat\"] = np.concatenate(out)\n",
    "        preds_df = pd.DataFrame(pdf_dict)\n",
    "        return preds_df\n",
    "\n",
    "\n",
    "def lin_reg(X, y, lmbda):\n",
    "    X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "    beta = np.linalg.solve(X.T @ X + lmbda * np.eye(X.shape[1]), X.T @ y)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_VAR(key_cols, support_cols, train_df):\n",
    "    prop_df = get_proportions(key_cols, support_cols, train_df)\n",
    "    prop_df, prev_prop_df = year_diff(key_cols, support_cols, prop_df)\n",
    "\n",
    "    def fit(x_df):\n",
    "        model = VARModel(lag=6, support_cols=support_cols)\n",
    "        model.fit(x_df, 0)\n",
    "        return model\n",
    "    return prop_df.groupby(key_cols).apply(fit).reset_index(), prev_prop_df\n",
    "\n",
    "\n",
    "def predict_VAR(key_cols, support_cols, test_df, var_models_df, prev_prop_df):\n",
    "    def predict(x_df):\n",
    "        days = x_df[\"ds\"].drop_duplicates().shape[0]\n",
    "        filter = pd.Series(var_models_df.shape[0] * [True])\n",
    "        for k in key_cols:\n",
    "            v = x_df[k].iloc[0]\n",
    "            filter = filter & (var_models_df[k] == v)\n",
    "        model = var_models_df[filter].iloc[0, -1]\n",
    "        return model.predict(test_df, days)\n",
    "\n",
    "    prop_hat_df = test_df.groupby(key_cols).apply(predict).reset_index()\n",
    "    return year_inv_diff(key_cols, support_cols, prop_hat_df, prev_prop_df)\n",
    "\n",
    "def top_down_cv(key_cols, support_cols, train_df, stores_dict, all_holidays_df, train_days=365*3, val_days=16, interval=64):\n",
    "    d0 = datetime.strptime(train_df[\"ds\"].iloc[0], \"%Y-%m-%d\").date()\n",
    "    d_max = datetime.strptime(train_df[\"ds\"].iloc[-1], \"%Y-%m-%d\").date()\n",
    "\n",
    "    def cv(x_df):\n",
    "        # x_df has all same key_cols but not grouped yet, contains y, dprop, and prev_prop\n",
    "\n",
    "        # retrieve h_df\n",
    "        store_nbrs = x_df[\"store_nbr\"].drop_duplicates()\n",
    "        states = [stores_dict[snbr][\"state\"] for snbr in store_nbrs]\n",
    "        cities = [stores_dict[snbr][\"city\"] for snbr in store_nbrs]\n",
    "        filter = (all_holidays_df[\"locale_name\"] == \"Ecuador\")\n",
    "        for s in states:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == s)\n",
    "        for c in cities:\n",
    "            filter = filter | (all_holidays_df[\"locale_name\"] == c)\n",
    "        h_df = all_holidays_df[filter]\n",
    "        h_df = h_df[[\"ds\", \"holiday\", \"lower_window\", \"upper_window\"]]\n",
    "\n",
    "        # aggregate x_df on key and support cols\n",
    "        x_df = x_df.groupby(key_cols + support_cols + [\"ds\"]).agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index().sort_values(key_cols + support_cols + [\"ds\"])\n",
    "        # get proportions and transform\n",
    "        x_prop_df = get_proportions(key_cols, support_cols, x_df)\n",
    "        x_prop_df, x_prev_prop_df = year_diff(key_cols, support_cols, x_prop_df)\n",
    "        x_prop_df = x_prop_df.sort_values(key_cols + support_cols + [\"ds\"])\n",
    "        x_prev_prop_df = x_prev_prop_df.sort_values(key_cols + support_cols + [\"ds\"])\n",
    "\n",
    "        # internal loop for each fold\n",
    "        # for loop\n",
    "        d = d0\n",
    "        errs = []\n",
    "        while d + timedelta(days=train_days) + timedelta(days=val_days - 1) <= d_max:\n",
    "            # calculate train dates and test dates\n",
    "            d1 = d.strftime(\"%Y-%m-%d\")\n",
    "            d2 = (d + timedelta(days=train_days)).strftime(\"%Y-%m-%d\")\n",
    "            d3 = (d + timedelta(days=train_days + val_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # retrieve agg train ys for prophet training\n",
    "            x_agg_train_df = x_df[(x_df[\"ds\"] >= d1) & (x_df[\"ds\"] < d2)].groupby(key_cols + [\"ds\"]).agg({\"y\":\"sum\", \"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "            # fit model\n",
    "            model = Prophet(uncertainty_samples=0, holidays=h_df)\n",
    "            model.add_regressor(\"onpromotion\")\n",
    "            model.add_regressor(\"dcoilwtico\")\n",
    "            model.fit(x_agg_train_df)\n",
    "            # retrieve agg val df for propher prediction\n",
    "            x_agg_val_df = x_df[(x_df[\"ds\"] >= d2) & (x_df[\"ds\"] < d3)].groupby(key_cols + [\"ds\"]).agg({\"onpromotion\":\"sum\", \"dcoilwtico\":\"first\"}).reset_index()\n",
    "            # predict\n",
    "            x_pred_df = model.predict(x_agg_val_df)[[\"ds\", \"yhat\"]]\n",
    "            x_pred_df[\"ds\"] = np.datetime_as_string(x_pred_df[\"ds\"].to_numpy(), unit='D')\n",
    "\n",
    "            # retrieve train props for var training\n",
    "            x_prop_train_df = x_prop_df[(x_prop_df[\"ds\"] >= d1) & (x_prop_df[\"ds\"] < d2)]\n",
    "            # fit var model and predict\n",
    "            var_model = VARModel(lag=8, support_cols=support_cols)\n",
    "            var_model.fit(x_prop_train_df, 1e-1)\n",
    "            x_prop_hat_df = var_model.predict(x_prop_df[(x_prop_df[\"ds\"] >= d2) & (x_prop_df[\"ds\"] < d3)], val_days)\n",
    "            # transform var predictions using prev_props\n",
    "            x_prop_hat_df = year_inv_diff([], support_cols, x_prop_hat_df, x_prev_prop_df)\n",
    "\n",
    "            # merge predictions and multiply to get final predictions\n",
    "            x_pred_df = x_prop_hat_df.merge(x_pred_df, on=\"ds\", how=\"left\")\n",
    "            x_pred_df[\"yhat\"] = (x_pred_df[\"yhat\"] * x_pred_df[\"prop_hat\"])\n",
    "            x_pred_df = x_pred_df.drop(\"prop_hat\", axis=1)\n",
    "            x_pred_df[\"yhat\"] = x_pred_df[\"yhat\"].clip(lower=0)\n",
    "\n",
    "            true_df = x_df[(x_df[\"ds\"] >= d2) & (x_df[\"ds\"] < d3)].drop(key_cols + [\"onpromotion\", \"dcoilwtico\"], axis=1)\n",
    "            x_pred_df = x_pred_df.merge(true_df, how=\"left\", on=support_cols + [\"ds\"])\n",
    "\n",
    "            err = msle(x_pred_df)\n",
    "            errs.append(err)\n",
    "\n",
    "            # calculate rmsle\n",
    "\n",
    "            # increment date\n",
    "            d += timedelta(days=interval)\n",
    "        return np.mean(np.array(errs))\n",
    "\n",
    "        # merge prophet predictions and var predictions, then multiply to retrieve final prediction\n",
    "    return train_df.groupby(key_cols).apply(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols = [\"cluster\"]\n",
    "support_cols = [\"family\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = all_cross_validation(key_cols, train_df, stores_dict, all_holidays_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = 4\n",
    "# prop_df = get_proportions(key_cols, support_cols, train_df)\n",
    "# prop_df, prev_prop_df = year_diff(key_cols, support_cols, prop_df)\n",
    "# x_df = prop_df[prop_df[\"cluster\"] == cluster]\n",
    "\n",
    "# model = VARModel(lag=6, support_cols=support_cols)\n",
    "# model.fit(x_df, 0)\n",
    "# prop_hat_df = model.predict(test_df, 16)\n",
    "# prop_hat_df[\"cluster\"] = cluster\n",
    "# year_inv_diff(key_cols, support_cols, prop_hat_df, prev_prop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_models_df, prev_prop_df = fit_VAR(key_cols, support_cols, train_df)\n",
    "# predict_VAR(key_cols, support_cols, test_df, var_models_df, prev_prop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:43:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:43:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:43:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:44:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:44:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:45:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:45:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:46:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:46:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "22:47:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:47:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/folders/70/9p60jp9112d1hs3c2p3vr1th0000gn/T/ipykernel_48296/2872040288.py:103: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return train_df.groupby(key_cols).apply(cv)\n"
     ]
    }
   ],
   "source": [
    "result_df = top_down_cv(key_cols, support_cols, train_df, stores_dict, all_holidays_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "1     0.771655\n",
       "2     0.797598\n",
       "3     0.769825\n",
       "4     0.607174\n",
       "5     0.985876\n",
       "6     0.747840\n",
       "7     1.307439\n",
       "8     1.025360\n",
       "9     0.637526\n",
       "10    0.829874\n",
       "11    0.973251\n",
       "12    0.717449\n",
       "13    0.806018\n",
       "14    1.109758\n",
       "15    0.863875\n",
       "16    1.374965\n",
       "17    0.849184\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
